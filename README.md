# LLM-Cache
基于Golang的高性能LLM语义缓存，能够大幅度减少LLM API开销，特别是在RAG-based 知识库系统当中
